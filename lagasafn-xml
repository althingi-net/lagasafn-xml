#!/usr/bin/env python3
import codecs
import json
import re
import roman
import signal
import os
import sys

from bs4 import BeautifulSoup
from collections import OrderedDict
from colorama import Fore
from colorama import Style
from lagasafn import diff_patch_utils
from lagasafn import settings
from lagasafn.contenthandlers import remove_ignorables
from lagasafn.pathing import make_xpath_from_node
from lagasafn.utils import sorted_law
from lagasafn.utils import terminal_width_and_height
from lagasafn.utils import write_xml
from lagasafn.settings import DATA_DIR
from lxml import etree
from lxml.builder import E
from multiprocessing import Pool
from reynir import NounPhrase
from sys import stderr
from lagasafn.parser import LawParser, parse_ambiguous_chapter, parse_ambiguous_section, parse_appendix, parse_article, parse_article_chapter, parse_chapter, parse_deletion_marker, parse_extra_docs, parse_footnotes, parse_law_number_and_date, parse_law_title, parse_minister_clause_footnotes, parse_numerical_article, parse_presidential_decree_preamble, parse_sentence_with_title, parse_subarticle, parse_subchapter, parse_table, postprocess_law

LAW_FILENAME = os.path.join(
    DATA_DIR, "original", settings.CURRENT_PARLIAMENT_VERSION, "%d%s.html"
)  # % (law_year, law_num)
CLEAN_FILENAME = os.path.join(
    DATA_DIR, "cleaned", "%d-%d.html"
)  # % (law_year, law_num)
PATCHED_FILENAME = os.path.join(
    DATA_DIR, "patched", "%d-%d.html"
)  # % (law_year, law_num)
PATCH_FILENAME = os.path.join(
    DATA_DIR, "patches", settings.CURRENT_PARLIAMENT_VERSION, "%d-%d.html.patch"
)  # % (law_year, law_num)
XML_FILENAME = os.path.join(DATA_DIR, "xml", "%d.%s.xml")  # % (law_year, law_num)
XML_INDEX_FILENAME = os.path.join(DATA_DIR, "xml", "index.xml")
XML_REFERENCES_FILENAME = os.path.join(DATA_DIR, "xml", "references.xml")

ERRORMAP_FILENAME = os.path.join("data", "json-maps", "errormap.json")


def clean_content(content):
    # Decode ISO-8859-1 character encoding.

    # content = content.decode('ISO-8859-1')

    # Make sure that horizontal bar tags are closed properly.
    # content = content.replace('<hr>', '<hr />')

    # Make sure that linebreak tags are closed properly.
    # content = content.replace('<br>', '<br />')

    if not settings.FEATURES["PARSE_MARKERS"]:
        # Remove markers for previous changes and removed content.
        content = content.replace("[", "").replace("]", "")
        content = content.replace("…", "").replace("&hellip;", "")

    # Remove links to website
    # strings_to_remove = (
    #     'Ferill málsins á Alþingi.',
    #     'Frumvarp til laga.',
    # )
    # for s in strings_to_remove:
    #     content = content.replace(s, '')

    # Make sure that image tags are closed properly.
    # e = re.compile(r'<img ([^>]*)>', re.IGNORECASE)
    # content = e.sub(r'<img \1 />', content)

    # Remove superscript/subscript when ratios are presented in the form of
    # divisions. For example, "3/4" tends to be stylized with a superscripted
    # "3" and a subscripted "4". We'll want to remove such styling because
    # we're only interested in content. Layouting mechanisms will have to
    # stylize them again if needed.
    e = re.compile(
        r'<sup style="font-size:60%">(\d+)</sup>/<span style="font-size:60%">(\d+)</span>'
    )
    content = e.sub(r"\1/\2", content)

    # Remove <a id=""> tags which are unclosed and seem without purpose.
    # For example, see source of page: http://www.althingi.is/altext/143/s/0470.html
    # content = content.replace('<a id="">', '')

    # Remove links to other laws. These are not useful in their current state.
    # Rather, references to other laws, such as "laga nr. XX/XXXX" or "lög nr.
    # XX/XXXX" should be automatically turned into precise references. (This
    # has not been implemented at the time of this writing. Remove this
    # comment when it has been. 2021-06-25)
    e = re.compile(r'<a href="\d{7}.html">(.*?)</a>')
    content = e.sub(r"\1", content)

    # Fix inconsistent deletion markers.
    # TODO: This only occurs in 160/2010 and should be removed once it has
    # been fixed in the data.
    e = re.compile(r"&hellip;<sup>(\d+)\)</sup>")
    content = e.sub(r'&hellip;<sup style="font-size:60%">\1)</sup>', content)

    # Find the law content and exit if it does not exist
    soup_law = BeautifulSoup(content, "html5lib").find("html")
    if soup_law is None:
        return None

    soup = BeautifulSoup(soup_law.__str__(), "html5lib")  # Parse the law content

    if not settings.FEATURES["PARSE_MARKERS"]:
        # Remove superscripts indicating previous change. Only removes them when
        # they are found outside of footnotes.
        superscripts = soup.find_all("sup")
        for s in superscripts:
            if s.parent.name != "small" and re.match(r"^\d{1,3}\)$", s.text):
                s.extract()

    # Remove tags entirely irrelevant to content
    # tags_to_remove = ['small'] # Previously also ['hr', 'script', 'noscript', 'head']
    # for target_tag in tags_to_remove:
    #     [s.extract() for s in soup(target_tag)]

    # Remove empty tags, but only if they're empty
    empty_tags_to_remove = ["p", "h2", "i"]
    for target_tag in empty_tags_to_remove:
        empty_tags = soup.find_all(
            lambda tag: tag.name == target_tag
            and not tag.contents
            and (tag.string is None or not tag.string.strip())
        )
        [empty_tag.extract() for empty_tag in empty_tags]

    # Keep consecutive <br />s only at 2 at most.
    # Commented because it got in the way of parsing stray text after numarts
    # (indicating a new paragraph). It does not seem to damage the XML and
    # there is no record of why this was done in the first place. Remove
    # entirely if 2022-10-01 was a long time ago.
    # brs_in_a_row = 0
    # all_tags = soup.find_all()
    # for t in all_tags:
    #    if t.name == 'br':
    #        if brs_in_a_row >= 2:
    #            t.extract()
    #            brs_in_a_row -= 1
    #        else:
    #            brs_in_a_row = brs_in_a_row + 1
    #    else:
    #        brs_in_a_row = 0

    # Replace <html> and <body> tags' with <div>s.
    """
    body_tag = soup.find('body')
    if body_tag is not None:
        body_tag.attrs['id'] = 'body_tag'
        body_tag.name = 'div'
    html_tag = soup.find('html')
    html_tag.attrs['id'] = 'html_tag'
    html_tag.name = 'div'
    """

    # Add charset tag
    charset_tag = soup.new_tag("meta", charset="utf-8")
    soup.insert(0, charset_tag)

    # Remove ignorables such as appendices and external documents that don't
    # belong to the law itself. The nature and format of these things are not
    # feasibly parsable.
    soup = remove_ignorables(soup)

    xhtml = soup.prettify()

    # Final cleanup at text-stage.
    xhtml = xhtml.replace(" <!-- Tab -->\n  ", "&nbsp;&nbsp;&nbsp;&nbsp;")

    return xhtml


def clean_law(law_num, law_year):
    with codecs.open(
        LAW_FILENAME % (law_year, str(law_num).zfill(3)), "r", "ISO-8859-1"
    ) as infile:
        raw_content = infile.read()
        infile.close()

    content = clean_content(raw_content)

    if content is None:
        print(" failed.")
        print("Error: Law %d/%d does not seem to exist" % (law_year, law_num))
        quit(1)

    if not os.path.isdir(os.path.dirname(CLEAN_FILENAME)):
        os.mkdir(os.path.dirname(CLEAN_FILENAME))

    with open(CLEAN_FILENAME % (law_year, law_num), "w") as clean_file:
        clean_file.write(content)
        clean_file.close()


def patch_law(law_num, law_year):
    if not os.path.isdir(os.path.dirname(PATCHED_FILENAME)):
        os.mkdir(os.path.dirname(PATCHED_FILENAME))

    filename = CLEAN_FILENAME % (law_year, law_num)
    patch_path = os.path.join(PATCH_FILENAME % (law_year, law_num))
    patched_content = diff_patch_utils.do_patch(filename, patch_path)
    with open(PATCHED_FILENAME % (law_year, law_num), "w") as patched_file:
        patched_file.write(patched_content)


def make_xml(law_num, law_year):
    parser = LawParser(law_num, law_year)
    law = parser.law
    lines = parser.lines
    matcher = parser.matcher
    occurrence_distance = parser.occurrence_distance

    # The cleaned document that we're processing is expected to put every tag,
    # both its opening and closing, on a separate line. This allows us to
    # browse the HTML contents on a per-line basis.
    for line in lines:
        line = line.strip()
        parser.line = line

        # Beware: The order of these function calls is sometimes important. 
        #         They are updating state based on guard clauses, and while
        #         the guard clauses *should* make this order irrelevant, 
        #         but let's not take any chances until we're certain.

        # Parse elements in the intro.
        parse_law_title(parser)
        parse_law_number_and_date(parser)
        parse_minister_clause_footnotes(parser)
        parse_presidential_decree_preamble(parser)

        # Intro is finished after here.
        # We'll early-escape all the following parser steps if we're not done
        # with the intro.
        #  
        #if not parser.trail_reached("intro-finished"): 
        #    continue

        parse_chapter(parser)

        # TODO: These two functions must happen here and must have the break condition, 
        # otherwise things break farther down the parse.
        # This means that these functions are predicated on a different style
        # of guard premise than the other functions. Worth reconsidering.
        if parse_extra_docs(parser) or parse_appendix(parser):
            break

        parse_subchapter(parser)
        parse_article_chapter(parser)
        parse_ambiguous_chapter(parser)
        parse_ambiguous_section(parser)
        parse_sentence_with_title(parser)
        parse_article(parser)
        parse_subarticle(parser)
        parse_numerical_article(parser)
        parse_deletion_marker(parser)
        
        # TODO: Check if we can reuse this logic within parse_subarticle
        parse_table(parser) 

        parse_footnotes(parser)


    ##########################################################################
    # At this point, the XML object `law` contains the entire document with
    # all its content. Now, we do some post-processing of the already existing
    # content. This is kept separate mostly for reasons of clarity.
    ##########################################################################
    postprocess_law(parser)

    # Write the XML object to output file.
    write_xml(law, XML_FILENAME % (parser.law_year, parser.law_num))


def process_law(law_id):
    try:
        (law_num, law_year) = law_id.split("/")
        law_num = int(law_num)
        law_year = int(law_year)

        clean_law(law_num, law_year)

        # Delete existing patch file so we don't accidentally have outdated
        # patched files when for example changing to another parliament version.
        patched_path = PATCHED_FILENAME % (law_year, law_num)
        if os.path.isfile(patched_path):
            os.unlink(patched_path)

        # check if we have patch for current law and parliament version
        patch_path = os.path.join(PATCH_FILENAME % (law_year, law_num))
        if os.path.isfile(patch_path):
            # use patch file to create patched version of cleaned file
            patch_law(law_num, law_year)

        try:
            make_xml(law_num, law_year)

        except BaseException:
            # Git is used to monitor success of parsing new versions of the legal
            # codex. When we fail at parsing a document, the accurate
            # representation is the absence of that document.
            try:
                os.remove(XML_FILENAME % (law_year, law_num))
            except FileNotFoundError:
                # This does not matter.
                pass

            # Make sure that this ends up being someone's problem.
            raise

        # Report back that law was processed without error.
        return [law_id, None]

    except Exception:
        # Something went wrong in processing the law, so we'll record the
        # traceback to the errormap.
        import traceback

        lines = "".join(traceback.format_exception(*sys.exc_info()))
        print("Error processing law %s: %s" % (law_id, lines))
        # Report back which law was attempted to process, and error.
        return [law_id, lines]


def get_available_law_ids():
    law_ids = []
    for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
        if re.match(r"^\d{7}\.html$", filename):
            law_year = int(filename[0:4])
            law_num = int(filename[4:7])
            law_id = "%d/%d" % (law_num, law_year)
            law_ids.append(law_id)
    return law_ids


def get_broken_law_ids():
    law_ids = []

    if os.path.isfile(ERRORMAP_FILENAME):
        with open(ERRORMAP_FILENAME, "r") as f:
            errormap = json.load(f, object_pairs_hook=OrderedDict)
            for law_id in errormap.keys():
                if errormap[law_id] is not None:
                    law_ids.append(law_id)

    return law_ids


# Displays what's going on to the terminal.
def report(law_id, i, law_count, msg):
    nr = str(i + 1)
    while len(nr) < 4:
        nr = " %s" % nr

    law_id_str = str(law_id)
    while len(law_id_str) < 8:
        law_id_str = " %s" % law_id_str

    if msg == "done":
        color = Fore.GREEN
    elif msg == "failed":
        color = Fore.RED
    else:
        raise Exception('Unknown message "%s"' % msg)

    print(
        "[%s/%d] %s %s%s%s" % (nr, law_count, law_id_str, color, msg, Style.RESET_ALL)
    )


# Displays errors known according to errormap.
def display_errors(law_ids):
    with open(ERRORMAP_FILENAME, "r") as f:
        errormap = json.load(f, object_pairs_hook=OrderedDict)

        # Counters.
        failures = 0
        successes = 0

        # The way we want to display this is basically the opposite of
        # how we want to store it. We shall invert the errormap.
        inverted_errormap = OrderedDict()
        for law_id in errormap.keys():
            # Ignore errors that are not a part of the laws currently
            # being requested for processing.
            if law_id not in law_ids:
                continue

            error_msg = errormap[law_id]

            if error_msg:
                failures += 1
            else:
                # If there is no error, it's success.
                successes += 1
                continue

            if error_msg not in inverted_errormap:
                inverted_errormap[error_msg] = []

            inverted_errormap[error_msg].append(law_id)

        # Sort errors so that the most common ones are shown last.
        inverted_errormap = OrderedDict(
            sorted(inverted_errormap.items(), key=lambda x: len(x[1]))
        )

        width, height = terminal_width_and_height()
        for error_msg in inverted_errormap.keys():
            print()
            print("%s" % ("-" * width))
            print(error_msg)
            print("Errors: %d" % len(inverted_errormap[error_msg]))
            print()
            for law_id in inverted_errormap[error_msg]:
                print(" - %s" % law_id, end="")

        print()
        print("%s" % ("-" * width))
        print(
            "Total: %d. Successes: %d. Failures: %d. Success ratio: %.2f%%."
            % (
                successes + failures,
                successes,
                failures,
                100 * (successes / (successes + failures)),
            )
        )


def update_index():
    print("Updating index...", end="", flush=True)

    # Index root element.
    index = E("index")

    # A place for statistics and such. These will be turned into XML nodes
    # later on but are consolidated here for clarity.
    stats = {
        "total-count": 0,
        "empty-count": 0,
        "non-empty-count": 0,
    }

    # Add a place to place the law entries.
    law_entries = E("law-entries")
    index.append(law_entries)

    # Iterate through all the XML files we have.
    for law_xml_filename in os.listdir(os.path.dirname(XML_FILENAME)):
        # Only interested in XML files.
        if law_xml_filename[-4:] != ".xml":
            continue

        # Determine the law's year and number from the XML file's name.
        try:
            law_year = int(law_xml_filename[0:4])
            law_nr = law_xml_filename[5 : law_xml_filename.find(".", 5)]
        except ValueError:
            # This is not a file in the correct name format. Probably the
            # index itself. We'll ignore it.
            continue

        # Read the XML file's content, from which we'll get a bunch of
        # information for the index.
        law = etree.parse(XML_FILENAME % (law_year, law_nr)).getroot()

        # Law-specific info for the index.
        identifier = f"{law_nr}/{law_year}"
        name = law.find("name").text
        date = law.find("num-and-date/date").text
        is_empty = law.getchildren()[-1].tag == "minister-clause"
        art_count = len(law.findall("art"))

        # Determine some structural statistics.
        chapter_count = len(law.xpath("/law/chapter"))
        subchapter_count = len(law.xpath("/law/chapter/subchapter"))
        art_count = len(law.xpath("/law/art"))
        if chapter_count > 0:
            art_count += len(law.xpath("/law/chapter/art"))
        if subchapter_count > 0:
            art_count += len(law.xpath("/law/chapter/subchapter/art"))

        # Determine the document's structure from the stats we've found.
        # NOTE: This method of consecutively appending path elements to the
        # `chapter_structure` may very well need to be revised when we begin
        # to support sub-chapters, super-chapters, different kinds of
        # chapters, since we will almost certainly run into much more
        # complicated scenarios than those that can be described in such a
        # hierarchical fashion.
        #
        # For example, lög nr. 42/1983 (152c) has a chapter for temporary
        # clauses, but is otherwise without chapters.
        structure = ""  # Undetermined
        if chapter_count > 0:
            structure += "/chapter"
        if subchapter_count > 0:
            structure += "/subchapter"
        if art_count > 0:
            structure += "/art"

        # Generate conjugated names of law.
        #
        # Most laws start with the string "Lög um" with everything following
        # already being in the accusative, so we really only need to conjugate
        # the "Lög um", which we can hard-code here since it's so predictable
        # within our linguistic context. External libraries are actually more
        # likely to make mistakes because they are unaware of the context, so
        # they may not even recognize the gender of "lög", for example, and may
        # conjugate parts of the law that we already know from context should
        # be in the accusative. Also, this is **way** faster than invoking all
        # the complications and corner-cases of Icelandic grammar.
        #
        # In other cases, the name ends with "lög" which we will conjugate
        # using an external library because those conjugations may include
        # complicated grammatical rules that we don't want to write ourselves.
        conjugation_success = False
        if name.find("Lög um") == 0:
            # The most common and predictable form.

            rest_of_name = name[7:]
            name_accusative = "Lög um %s" % rest_of_name
            name_dative = "Lögum um %s" % rest_of_name
            name_genitive = "Laga um %s" % rest_of_name

            conjugation_success = True
        else:
            # Things are a bit complicated now. Invoking external library.
            name_phrase = NounPhrase(name)
            name_accusative = name_phrase.accusative
            name_dative = name_phrase.dative
            name_genitive = name_phrase.genitive

            # At the time of this writing (2024-02-29, version 153c) only 3
            # examples of failed conjugations exist:
            #
            # - Tilskipun um fardaga presta á Íslandi og um réttindi þau, er
            #   prestur sá, sem frá brauði fer, eður erfingjar hans og einkum
            #   ekkjan eiga heimting á
            #
            # - Lög viðvíkjandi nafnbreyting Vinnuveitendafélags Íslands
            #
            # - Konungsbréf (til stiftamtm. og amtm.) um fiskiútveg á Íslandi
            #
            # All of them are empty or irrelevant so we won't deal with them
            # further. If necessary, they can be hard-coded since new laws with
            # this problem are extremely unlikely to ever come into effect.
            conjugation_success = name_phrase.parsed

        # We also make the first letter of the conjugated form lowercase to
        # make comparison easier, since conjugated forms are never in the
        # beginning of a sentence.
        if conjugation_success:
            name_accusative = name_accusative[0].lower() + name_accusative[1:]
            name_dative = name_dative[0].lower() + name_dative[1:]
            name_genitive = name_genitive[0].lower() + name_genitive[1:]

        # Append the gathered data to the index.
        law_entry = E(
            "law-entry",
            {
                "identifier": identifier,
                "nr": law_nr,
                "year": str(law_year),
                "date": date,
            },
            E("name", name),
            E(
                "name-conjugated",
                {
                    "success": "true" if conjugation_success else "false",
                },
                E("nomenative", name),
                E("accusative", name_accusative or ""),
                E("dative", name_dative or ""),
                E("genitive", name_genitive or ""),
            ),
            E(
                "meta",
                E("is-empty", "true" if is_empty else "false"),
                E("structure", structure),
                E("chapter-count", str(chapter_count)),
                E("art-count", str(art_count)),
            ),
        )

        law_entries.append(law_entry)
        print(".", end="", flush=True)

        # Add to stats. We are not sure what the rendering client is
        # interested in, so we'll leave a few variables for convenience, even
        # if they are easily calculable. The client should not need to
        # calculate basic things.
        stats["total-count"] += 1
        if is_empty:
            stats["empty-count"] += 1
        else:
            stats["non-empty-count"] += 1

    def number_sorter(number):
        """
        Used for properly sorting numbers in a string, so that "7" comes before
        "70" by prepending zeroes.
        """
        result = str(number)
        while len(result) < 3:
            result = "0%s" % result
        return result

    # Sort the law entries by year and number.
    law_entries[:] = reversed(
        sorted(
            law_entries,
            key=lambda law: (
                law.attrib["year"],
                number_sorter(law.attrib["nr"]),
            ),
        )
    )

    # Insert statistics into XML.
    node_stats = E("stats")
    index.insert(0, node_stats)
    for stat_key in stats.keys():
        node_stats.append(E(stat_key, str(stats[stat_key])))

    write_xml(index, XML_INDEX_FILENAME, skip_prettyprint_hack=True)

    print(" done")


def parse_references():
    print("Parsing references...", end="", flush=True)

    # These will record statistics as we run through the data, applied to the
    # XML afterwards.
    stat_loop_count = stat_conclusive_count = stat_inconclusive_count = 0
    xml_ref_doc = E("references")

    index = etree.parse(XML_INDEX_FILENAME).getroot()

    xpath_entry_selector = "/index/law-entries/law-entry"

    # This keeps track of sentences that we haven't figured out how to deal
    # with yet. We can use them to incrementally improve the reference
    # detection mechanism. Note however that just because it's empty,
    # doesn't mean that every possible reference has been detected.
    problems = {
        "conjugation-not-found": [],
    }

    # First iterate all the law entries to find all dative forms, which we will
    # need in its entirety later, when parsing the law.
    conjugated = {}
    for law_entry in index.xpath(xpath_entry_selector):
        name_accusative = law_entry.xpath("./name-conjugated/accusative")[0].text
        name_dative = law_entry.xpath("./name-conjugated/dative")[0].text
        name_genitive = law_entry.xpath("./name-conjugated/genitive")[0].text
        nr_and_year = "%s/%s" % (law_entry.attrib["nr"], law_entry.attrib["year"])
        conjugated[nr_and_year] = {
            "accusative": name_accusative,
            "dative": name_dative,
            "genitive": name_genitive,
        }
        del nr_and_year
        del name_genitive
        del name_dative
        del name_accusative

    # Words that we can use to conclusively show that we are at the end of
    # parsing the inner part of a reference.
    conclusives = [
        "auglýsingaskyldu",
        "á",
        "ákvæði",
        "ákvæðum",
        "einnig",
        "eftir",
        "fyrir",
        "gegn",
        "gilda",
        "grundvelli",
        "í",
        "með",
        "meðal annars",
        "né",
        "reglum",
        "samkvæmt",
        "samræmast",
        "sbr.",
        "skilningi",
        "skilyrði",
        "skilyrðum",
        "skv.",
        "undanþegnar",
        "undir",
        "við",
    ]

    # Patterns describing node-delimited parts of inner references.
    # IMPORTANT: The entire things must be in a group because the length of the
    # entire match is needed in code below.
    inner_reference_patterns = [
        r"(([A-Z]{1,9}\.?[-–])?([A-Z]{1,9})\. kafla( [A-Z])?)$",  # `chapter`
        r"(([A-Z]{1,9}\.?[-–])?([A-Z]{1,9})\. hluta( [A-Z])?)$",  # `chapter`-ish
        r"(((\d{1,3})\.[-–] ?)?(\d{1,3})\. gr\.( [a-z])?(,)?)$",  # `art`
        r"(((\d{1,3})\.[-–] ?)?(\d{1,3})\. mgr\.)$",  # `subart`
        r"(((\d{1,3})\.[-–] ?)?(\d{1,3})\. tölul\.)$",  # `numart`
        r"(([a-zA-Z][-–])?[a-zA-Z][-–]lið(ar)?)$",  # `numart` (alphabetic)
        r"(((\d{1,3})\.[-–] ?)?(\d{1,3})\. málsl\.)$",  # `sen`
    ]

    # Patterns describing parts that may appear before "og" but don't conform
    # to the `inner_reference_patterns` above.
    #
    # Examples:
    #     a- og c–i-lið 9. tölul. 14. og 15. gr.
    #     1., 2. eða 5. mgr. 109. gr.
    #
    # IMPORTANT: The entire things must be in a group because the length of the
    # entire match is needed in code below.
    and_inner_reference_patterns = [
        r"(([A-Za-z])[-–])$",
        r"(((\d{1,3})\., )*(\d{1,3})\.)$",
        r"(([A-Z]{1,9})\.)$",
    ]

    # Now iterate through all the laws and parse their contents.
    for law_entry in index.xpath(xpath_entry_selector):
        law_nr = law_entry.attrib["nr"]
        law_year = int(law_entry.attrib["year"])

        law = etree.parse(XML_FILENAME % (law_year, law_nr)).getroot()
        sens = law.xpath("//sen[not(ancestor::footnotes)]")

        law_ref_entry = E(
            "law-ref-entry", {"law-nr": str(law_nr), "law-year": str(law_year)}
        )

        for sen in sens:
            # We'll be butchering this so better make a copy.
            chunk = sen.text or ""

            # The outer references we'll be looking for (i.e.
            # `nr\. (\d{1,3}\/\d{4})`), may appear multiple times in the same
            # sentence. This will confuse the relevant searching mechanism by
            # it finding again an outer reference that it has already found.
            #
            # To remedy this, we will keep an offset which will always be
            # either 0 as defined here, or equal to the last
            # `potentials_outer_end`, so that we continue the search for the
            # outer reference from where we left off in the previous iteration,
            # rather than from the start of the chunk every time.
            potentials_outer_start_offset = 0

            # FIXME: Rename `pattern` and `matches`, they are too generic.
            pattern = r"(?<!EES-nefndarinnar )(?<!auglýsingu )nr\. (\d{1,3}\/\d{4})"
            matches = re.findall(pattern, chunk)

            for nr_and_year in matches:

                # NOTE: Every law has a name, so if this doesn't exist,
                # `nr_and_year` cannot refer to a law unless there's a mistake
                # in the legal codex itself.
                #
                # TODO: We might want to collect these and figure out if there
                # are any such mistakes, or if we can further utilize these
                # references even if they don't refer to laws.
                if nr_and_year not in conjugated:
                    problems["conjugation-not-found"].append(sen)
                    continue

                accusative = conjugated[nr_and_year]["accusative"]
                dative = conjugated[nr_and_year]["dative"]
                genitive = conjugated[nr_and_year]["genitive"]

                # The outer and inner references we will build.
                reference = ""

                # Becomes True if the underlying mechanism is able to
                # conclusively determine that the inner reference has been
                # parsed completely and correctly. This typically happens when
                # we run into key words like "skv." or "sbr.".
                certain_about_inner = False

                # Possible permutations of how the outer part of reference
                # might be construed.
                # FIXME: This might be better turned into a regex.
                potential_start_guesses = [
                    "%s, nr. %s" % (accusative, nr_and_year),
                    "%s, nr. %s" % (dative, nr_and_year),
                    "%s, nr. %s" % (genitive, nr_and_year),
                    "%s nr. %s" % (accusative, nr_and_year),
                    "%s nr. %s" % (dative, nr_and_year),
                    "%s nr. %s" % (genitive, nr_and_year),
                    "lög nr. %s" % nr_and_year,
                    "lögum nr. %s" % nr_and_year,
                    "laga nr. %s" % nr_and_year,
                ]

                # A string holding potential parts of an inner reference. Gets
                # continuously analyzed and chipped away at, as we try to
                # include as much as we can.
                potentials = ""

                # NOTE: We call this a starting location, but we're parsing
                # backwards into the string from the starting location, even
                # though we're looking for the starting location forward.
                potentials_outer_start = -1
                potentials_outer_end = -1

                # Begin by finding out where the next outer reference starts.
                # The `found_guess` variable is used after the loop to figure
                # out where it ends and to decide the search offset in the next
                # round of scanning the chunk.
                for potential_start_guess in potential_start_guesses:
                    attempt = chunk.find(
                        potential_start_guess, potentials_outer_start_offset
                    )

                    if potentials_outer_start == -1 or (
                        attempt > -1 and attempt < potentials_outer_start
                    ):
                        potentials_outer_start = attempt

                        # Record the outer end location so that we can decide
                        # where to end the label constructed later.
                        potentials_outer_end = attempt + len(potential_start_guess)

                # Remember where to pick up the search for different
                # permutations of outer references, in the next outer reference
                # to be processed.
                potentials_outer_start_offset = potentials_outer_end

                if potentials_outer_start > -1:

                    # Potentials are the string that potentially contains the
                    # inner parts of a reference, i.e. without the law's number
                    # and name.
                    potentials = chunk[:potentials_outer_start].strip()

                    # Check for inner parts of reference for as long as we can.
                    # FIXME: This `beendone` loop-checking thing is ridiculous.
                    # We should remove things from `potentials` until it's
                    # empty and break there.
                    beendone = 0
                    while True:

                        # Check for inner reference components by iterating
                        # through known patterns.
                        for inner_pattern in inner_reference_patterns:
                            matches = re.findall(inner_pattern, potentials)
                            if len(matches) > 0:
                                match = matches[0][0]
                                reference = "%s %s" % (match, reference)
                                potentials = potentials[: -len(match)].strip()

                                del match
                                continue

                        # Check if we can prove that we're done by using
                        # conclusive words.
                        for conclusion_part in conclusives:
                            p_len = len(potentials)
                            cp_len = len(conclusion_part)
                            if (
                                p_len >= cp_len
                                and potentials[-cp_len:].lower() in conclusives
                            ):
                                certain_about_inner = True
                                break
                        if certain_about_inner:
                            # Break outer loop as well.
                            break

                        # Check if we can prove that we're done by seeing if
                        # another outer reference can be found right before.
                        preceeding_law_match = re.findall(
                            r"(nr\. (\d{1,3}/\d{4}),)$", potentials
                        )
                        if len(preceeding_law_match):
                            pl_len = len(preceeding_law_match[0][0])
                            potentials = potentials[-pl_len:]
                            certain_about_inner = True
                            break

                        # Check for separators "og" and "eða".
                        # This could be done in a loop but that would just add
                        # code indenting.
                        separator = ""
                        for sep_try in ["og", "eða"]:
                            if len(potentials) < len(sep_try):
                                continue

                            if potentials[-len(sep_try) :] == sep_try:
                                separator = sep_try
                                break

                        if len(separator):
                            # If we run into a separator with no inner
                            # reference, then this is a reference to the entire
                            # law and not to an article within it. We may
                            # safely assume that we're done parsing.
                            if len(reference) == 0:
                                certain_about_inner = True
                                break

                            # The string ", " preceeding the separator
                            # indicates that what comes before is not a part of
                            # the same reference. We determine that the
                            # reference is complete.
                            if potentials.rstrip(separator)[-2:] == ", ":
                                certain_about_inner = True
                                break

                            # Otherwise, add matched separator and continue.
                            reference = "%s %s" % (separator, reference)
                            potentials = potentials[: -len(separator)].strip()

                            # Parts before separators (such "og" or "eða")
                            # follow a different format, because their context
                            # is determined by what comes after them.
                            #
                            # Consider:
                            #     a- og c–i-lið 9. tölul.
                            #
                            # The "a-" part doesn't make any sense except in
                            # the context of "c-i-lið" that comes afterward. We
                            # don't normally parse "a-" individually, we only
                            # parse it specifically after we run into an "og".
                            for and_inner_pattern in and_inner_reference_patterns:
                                matches = re.findall(and_inner_pattern, potentials)
                                if len(matches) > 0:
                                    match = matches[0][0]
                                    reference = "%s %s" % (match, reference)
                                    potentials = potentials[: -len(match)].strip()
                                    del match

                                del matches

                            # We don't need to concern ourselves more with this
                            # iteration. Moving on.
                            continue

                        # If we can't find any matches anymore, it means that
                        # we're out of the inner part of the reference or into
                        # something that we don't support yet.
                        if beendone > 100:
                            print("\n[ LOOP DETECTED ]: %s" % potentials)
                            stat_loop_count += 1
                            break
                        beendone += 1

                    # Indentation of inner reference being finished.
                    pass

                # May contain stray whitespace due to concatenation.
                reference = reference.strip()

                # The visible part of the text that would normally be
                # expected to be a link on a web page.
                link_label = ""
                if len(reference) > 0:
                    link_label = chunk[
                        chunk.rfind(
                            reference, 0, potentials_outer_end
                        ) : potentials_outer_end
                    ]
                else:
                    link_label = chunk[potentials_outer_start:potentials_outer_end]

                # Generate an XPath to the current node containing the
                # reference.
                location = make_xpath_from_node(sen)

                if certain_about_inner:

                    # To adhere to our established norm of separating law
                    # number and law year.
                    target_law_nr, target_law_year = nr_and_year.split("/")

                    # Either find or construct the node for the given entry.
                    ref_node = law_ref_entry.find('node[@location="%s"]' % location)
                    if ref_node is None:
                        ref_node = E(
                            "node",
                            {
                                "location": location,
                                "text": chunk,
                            },
                        )
                        law_ref_entry.append(ref_node)

                    ref_node.append(
                        E(
                            "reference",
                            {
                                "link-label": link_label,
                                "inner-reference": reference,
                                "law-nr": target_law_nr,
                                "law-year": target_law_year,
                            },
                        )
                    )
                    xml_ref_doc.append(law_ref_entry)

                    stat_conclusive_count += 1

                else:
                    print("-------------")
                    print("- Processing: %s/%s" % (law_nr, law_year))
                    print("- Chunk:      %s" % chunk)
                    print("- Potentials: %s" % potentials)
                    print("- Law:        %s" % nr_and_year)
                    print("- Accusative: %s" % accusative)
                    # print("Dative: %s" % dative)  # Unimplemented, but we need it.
                    print("- Genitive:   %s" % genitive)
                    print("- Reference:  %s" % reference)
                    print(
                        "- Conclusive: %s"
                        % ("true" if certain_about_inner else "false")
                    )

                    stat_inconclusive_count += 1

            # Indent for: `for nr_and_year in matches`
            pass

        print(".", end="", flush=True)

    # Apply statistics to XML.
    xml_ref_doc.attrib["stat-loop-count"] = str(stat_loop_count)
    xml_ref_doc.attrib["stat-inconclusive-count"] = str(stat_inconclusive_count)
    xml_ref_doc.attrib["stat-conclusive-count"] = str(stat_conclusive_count)
    write_xml(xml_ref_doc, XML_REFERENCES_FILENAME, skip_prettyprint_hack=True)

    print(" done")


def usage(exec_name, message=None):
    print(
        "Usage: %s [law_number>/<year>] [law_number>/<year>]..." % exec_name,
        file=stderr,
    )
    print(file=stderr)
    print(
        "Running without options or specific laws will process all available laws.",
        file=stderr,
    )
    print(file=stderr)
    print("Options:", file=stderr)
    print("    --help                    Display this help message.", file=stderr)
    print(file=stderr)
    print(
        "    --try-broken              Try processing laws known to have failed.",
        file=stderr,
    )
    print(
        "    --single-thread           Skip multi-processing (mostly for debugging).",
        file=stderr,
    )
    print(
        "    --rebuild-straytextmap    Force the asking of user about stray text following numarts",
        file=stderr,
    )
    print(file=stderr)
    print("Index options:", file=stderr)
    print(
        "    --index-only              Don't process laws or parse references, only update the index.",
        file=stderr,
    )
    print(
        "    --skip-index              Skip updating index.",
        file=stderr,
    )
    print(file=stderr)
    print("Reference parsing options:", file=stderr)
    print(
        "    --references-only         Don't process laws or update index, only parse references.",
        file=stderr,
    )
    print("    --skip-references         Skip the parsing of references.", file=stderr),
    print(file=stderr)
    if message:
        print("Error: %s" % message, file=stderr)
    quit(1)


def main(argv):
    # A list of valid options so that we'll know when something is thrown in
    # that we don't know what to do with.
    valid_options = [
        "--help",
        "--try-broken",
        "--single-thread",
        "--rebuild-straytextmap",
        "--index-only",
        "--skip-index",
        "--references-only",
        "--skip-references",
    ]

    if "--help" in argv:
        usage(argv[0])

    # Only update the index and immediately quit if so requested.
    if "--index-only" in argv:
        update_index()
        quit()
    elif "--references-only" in argv:
        parse_references()
        quit()

    # Container for laws that are about to be processed.
    law_ids = []

    if "--try-broken" in argv:
        law_ids = get_broken_law_ids()

    # Add things in command line that match the pattern for a law_id.
    for arg in argv[1:]:
        if re.match(r"^\d{1,4}\/\d{4}$", arg):
            law_ids.append(arg)
        elif arg in valid_options:
            settings.options[arg] = True
        elif arg not in valid_options:
            usage(argv[0], 'Unknown option "%s"' % arg)

    # If nothing is selected, we'll process everything.
    if len(law_ids) == 0:
        law_ids = get_available_law_ids()

    # Sort the law_ids in their own special way (year first, num second).
    law_ids = sorted_law(law_ids)

    # This is apparently safer than `multiprocessing.cpu_count()`,
    # according to:
    # https://stackoverflow.com/questions/1006289/how-to-find-out-the-number-of-cpus-using-python
    cpu_count = len(os.sched_getaffinity(0))

    def init_pool():
        # Start ignoring the KeyboardInterrupt signal in the main thread. The
        # result is that it gets caught by the sub-processes, which **don't**
        # inherit this setting. The exception is then thrown when waiting for
        # the process pool to finish, and caught by the code running the
        # `main` function.
        signal.signal(signal.SIGINT, signal.SIG_IGN)

    with Pool(cpu_count, init_pool) as pool:
        if "--single-thread" in argv:
            # Multiprocessing has some wide-reaching implications for various
            # under-the-hood mechanics like debugging. We need the ability to
            # have those work, so we offer this option to sidestep threading.
            #
            # This is only "yield-ified" so that the results stay compatible
            # with the code that handles those same results when using
            # multiprocessing.
            def yieldify_processing(law_ids):
                for law_id in law_ids:
                    yield process_law(law_id)

            results = yieldify_processing(law_ids)
        else:
            # Start an asynchronous pool of processes, as many as there are
            # CPUS, giving them a list of the laws that need processing.
            results = pool.imap_unordered(process_law, law_ids)

        # Open the errormap for recording successes and errors.
        with open(ERRORMAP_FILENAME, "r") as f:
            errormap = json.load(f, object_pairs_hook=OrderedDict)

        # Monitor and record return successes and errors. We'll want to record
        # the errormap, no matter what.
        try:
            # Initial state of iterator.
            i = 0

            # Let's only do this once.
            law_count = len(law_ids)

            # Keep doing this until we hit a StopIteration exception.
            while True:
                try:
                    # Catch next result.
                    law_id, error_trace = next(results)

                    # Remember in errormap (gets written later).
                    errormap[law_id] = error_trace

                    # Tell the user about it.
                    msg = "done" if error_trace is None else "failed"
                    report(law_id, i, law_count, msg)

                    # Increase iterator.
                    i += 1

                except StopIteration:
                    break
        finally:
            # Write the errormap.
            with open(ERRORMAP_FILENAME, "w") as f:
                json.dump(errormap, f)

    # Update the index (unless skipped).
    if "--skip-index" not in argv:
        update_index()

    # Parse references (unless skipped).
    if "--skip-references" not in argv:
        parse_references()

    # List the errors and identify laws in which they occurred.
    display_errors(law_ids)


try:
    main(sys.argv)
except KeyboardInterrupt:
    quit()
except Exception as e:
    if settings.DEBUG:
        raise
    else:
        print("Error: %s" % e, file=stderr)
