#!/usr/bin/env python3
import codecs
import json
import re
import signal
import os
import sys

from bs4 import BeautifulSoup
from collections import OrderedDict
from colorama import Fore
from colorama import Style
from lagasafn import diff_patch_utils
from lagasafn import settings
from lagasafn.constants import LAW_FILENAME
from lagasafn.constants import ERRORMAP_FILENAME
from lagasafn.constants import CLEAN_FILENAME
from lagasafn.constants import PATCHED_FILENAME
from lagasafn.constants import PATCH_FILENAME
from lagasafn.constants import XML_INDEX_FILENAME
from lagasafn.constants import XML_FILENAME
from lagasafn.contenthandlers import remove_ignorables
from lagasafn.references import parse_references
from lagasafn.utils import sorted_law
from lagasafn.utils import terminal_width_and_height
from lagasafn.utils import write_xml
from lxml import etree
from lxml.builder import E
from multiprocessing import Pool
from reynir import NounPhrase
from sys import stderr
from lagasafn.parser import LawParser
from lagasafn.parser import parse_intro
from lagasafn.parser import parse_ambiguous_chapter
from lagasafn.parser import parse_ambiguous_section
from lagasafn.parser import parse_appendix
from lagasafn.parser import parse_article
from lagasafn.parser import parse_article_chapter
from lagasafn.parser import parse_chapter
from lagasafn.parser import parse_deletion_marker
from lagasafn.parser import parse_extra_docs
from lagasafn.parser import parse_numerical_article
from lagasafn.parser import parse_sentence_with_title
from lagasafn.parser import parse_subarticle
from lagasafn.parser import parse_subchapter
from lagasafn.parser import parse_table
from lagasafn.parser import postprocess_law
from lagasafn.parse_footnotes import parse_footnotes
from lagasafn.parse_footnotes import parse_footnote

VERBOSE_MODE = False


def clean_content(content):
    # Decode ISO-8859-1 character encoding.

    # content = content.decode('ISO-8859-1')

    # Make sure that horizontal bar tags are closed properly.
    # content = content.replace('<hr>', '<hr />')

    # Make sure that linebreak tags are closed properly.
    # content = content.replace('<br>', '<br />')

    if not settings.FEATURES["PARSE_MARKERS"]:
        # Remove markers for previous changes and removed content.
        content = content.replace("[", "").replace("]", "")
        content = content.replace("…", "").replace("&hellip;", "")

    # Remove links to website
    # strings_to_remove = (
    #     'Ferill málsins á Alþingi.',
    #     'Frumvarp til laga.',
    # )
    # for s in strings_to_remove:
    #     content = content.replace(s, '')

    # Make sure that image tags are closed properly.
    # e = re.compile(r'<img ([^>]*)>', re.IGNORECASE)
    # content = e.sub(r'<img \1 />', content)

    # Remove superscript/subscript when ratios are presented in the form of
    # divisions. For example, "3/4" tends to be stylized with a superscripted
    # "3" and a subscripted "4". We'll want to remove such styling because
    # we're only interested in content. Layouting mechanisms will have to
    # stylize them again if needed.
    e = re.compile(
        r'<sup style="font-size:60%">(\d+)</sup>/<span style="font-size:60%">(\d+)</span>'
    )
    content = e.sub(r"\1/\2", content)

    # Remove <a id=""> tags which are unclosed and seem without purpose.
    # For example, see source of page: http://www.althingi.is/altext/143/s/0470.html
    # content = content.replace('<a id="">', '')

    # Remove links to other laws. These are not useful in their current state.
    # Rather, references to other laws, such as "laga nr. XX/XXXX" or "lög nr.
    # XX/XXXX" should be automatically turned into precise references. (This
    # has not been implemented at the time of this writing. Remove this
    # comment when it has been. 2021-06-25)
    e = re.compile(r'<a href="\d{7}.html">(.*?)</a>')
    content = e.sub(r"\1", content)

    # Fix inconsistent deletion markers.
    # TODO: This only occurs in 160/2010 and should be removed once it has
    # been fixed in the data.
    e = re.compile(r"&hellip;<sup>(\d+)\)</sup>")
    content = e.sub(r'&hellip;<sup style="font-size:60%">\1)</sup>', content)

    # Find the law content and exit if it does not exist
    soup_law = BeautifulSoup(content, "html5lib").find("html")
    if soup_law is None:
        return None

    soup = BeautifulSoup(soup_law.__str__(), "html5lib")  # Parse the law content

    if not settings.FEATURES["PARSE_MARKERS"]:
        # Remove superscripts indicating previous change. Only removes them when
        # they are found outside of footnotes.
        superscripts = soup.find_all("sup")
        for s in superscripts:
            if s.parent.name != "small" and re.match(r"^\d{1,3}\)$", s.text):
                s.extract()

    # Remove tags entirely irrelevant to content
    # tags_to_remove = ['small'] # Previously also ['hr', 'script', 'noscript', 'head']
    # for target_tag in tags_to_remove:
    #     [s.extract() for s in soup(target_tag)]

    # Remove empty tags, but only if they're empty
    empty_tags_to_remove = ["p", "h2", "i"]
    for target_tag in empty_tags_to_remove:
        empty_tags = soup.find_all(
            lambda tag: tag.name == target_tag
            and not tag.contents
            and (tag.string is None or not tag.string.strip())
        )
        [empty_tag.extract() for empty_tag in empty_tags]

    # Keep consecutive <br />s only at 2 at most.
    # Commented because it got in the way of parsing stray text after numarts
    # (indicating a new paragraph). It does not seem to damage the XML and
    # there is no record of why this was done in the first place. Remove
    # entirely if 2022-10-01 was a long time ago.
    # brs_in_a_row = 0
    # all_tags = soup.find_all()
    # for t in all_tags:
    #    if t.name == 'br':
    #        if brs_in_a_row >= 2:
    #            t.extract()
    #            brs_in_a_row -= 1
    #        else:
    #            brs_in_a_row = brs_in_a_row + 1
    #    else:
    #        brs_in_a_row = 0

    # Replace <html> and <body> tags' with <div>s.
    """
    body_tag = soup.find('body')
    if body_tag is not None:
        body_tag.attrs['id'] = 'body_tag'
        body_tag.name = 'div'
    html_tag = soup.find('html')
    html_tag.attrs['id'] = 'html_tag'
    html_tag.name = 'div'
    """

    # Add charset tag
    charset_tag = soup.new_tag("meta", charset="utf-8")
    soup.insert(0, charset_tag)

    # Remove ignorables such as appendices and external documents that don't
    # belong to the law itself. The nature and format of these things are not
    # feasibly parsable.
    soup = remove_ignorables(soup)

    xhtml = soup.prettify()

    # Final cleanup at text-stage.
    xhtml = xhtml.replace(" <!-- Tab -->\n  ", "&nbsp;&nbsp;&nbsp;&nbsp;")

    return xhtml


def clean_law(law_num, law_year):
    with codecs.open(
        LAW_FILENAME % (law_year, str(law_num).zfill(3)), "r", "ISO-8859-1"
    ) as infile:
        raw_content = infile.read()
        infile.close()

    content = clean_content(raw_content)

    if content is None:
        print(" failed.")
        print("Error: Law %d/%d does not seem to exist" % (law_year, law_num))
        quit(1)

    if not os.path.isdir(os.path.dirname(CLEAN_FILENAME)):
        os.mkdir(os.path.dirname(CLEAN_FILENAME))

    with open(CLEAN_FILENAME % (law_year, law_num), "w") as clean_file:
        clean_file.write(content)
        clean_file.close()


def patch_law(law_num, law_year):
    if not os.path.isdir(os.path.dirname(PATCHED_FILENAME)):
        os.mkdir(os.path.dirname(PATCHED_FILENAME))

    filename = CLEAN_FILENAME % (law_year, law_num)
    patch_path = os.path.join(PATCH_FILENAME % (law_year, law_num))
    patched_content = diff_patch_utils.do_patch(filename, patch_path)
    with open(PATCHED_FILENAME % (law_year, law_num), "w") as patched_file:
        patched_file.write(patched_content)


def make_xml(law_num, law_year):
    parser = LawParser(law_num, law_year)
    global VERBOSE_MODE
    if VERBOSE_MODE:
        print("Parsing law %d/%d" % (law_num, law_year))
        parser.verbosity = 1

    law = parser.law
    lines = parser.lines
    matcher = parser.matcher
    occurrence_distance = parser.occurrence_distance

    parse_intro(parser)

    # print("Left intro after line %s" % parser.lines.current_line_number)
    # The cleaned document that we're processing is expected to put every tag,
    # both its opening and closing, on a separate line. This allows us to
    # browse the HTML contents on a per-line basis.
    if not parser.trail_reached("intro-finished"):
        parser.error("Intro not finished after line %s. Aborting!" % parser.lines.current_line_number)
        return

    parser.enter("content")

    for line in lines:
        line = line.strip()
        if line == "<br/>":
            continue
        parser.note("Top level enter [%s]: %s" % (parser.lines.current_line_number, line))

        # Beware: The order of these function calls is sometimes important.
        #         They are updating state based on guard clauses, and while
        #         the guard clauses *should* make this order irrelevant,
        #         but let's not take any chances until we're certain.

        # TODO: These two functions must happen here and must have the break condition,
        # otherwise things break farther down the parse.
        # This means that these functions are predicated on a different style
        # of guard premise than the other functions. Worth reconsidering.
        if parse_extra_docs(parser) or parse_appendix(parser):
            break

        parse_chapter(parser)
        parse_subchapter(parser)
        parse_article_chapter(parser)
        parse_ambiguous_chapter(parser)
        parse_ambiguous_section(parser)
        parse_sentence_with_title(parser)
        parse_article(parser)
        parse_subarticle(parser)
        parse_numerical_article(parser)
        parse_footnotes(parser)
        parse_deletion_marker(parser)

        # TODO: Check if we can reuse this logic within parse_subarticle
        parse_table(parser)

        l = parser.lines.current_line
        if l == None:
            parser.note("Top level leave [%s] with NONE" % parser.lines.current_line_number)
        else:
            parser.note("Top level leave [%s]: %s" % (parser.lines.current_line_number, l.strip()))

        # parse_footnote(parser)

    parser.leave("content")
    ##########################################################################
    # At this point, the XML object `law` contains the entire document with
    # all its content. Now, we do some post-processing of the already existing
    # content. This is kept separate mostly for reasons of clarity.
    ##########################################################################
    postprocess_law(parser)

    # Write the XML object to output file.
    write_xml(law, XML_FILENAME % (parser.law_year, parser.law_num))


def process_law(law_id):
    try:
        (law_num, law_year) = law_id.split("/")
        law_num = int(law_num)
        law_year = int(law_year)

        clean_law(law_num, law_year)

        # Delete existing patch file so we don't accidentally have outdated
        # patched files when for example changing to another parliament version.
        patched_path = PATCHED_FILENAME % (law_year, law_num)
        if os.path.isfile(patched_path):
            os.unlink(patched_path)

        # check if we have patch for current law and parliament version
        patch_path = os.path.join(PATCH_FILENAME % (law_year, law_num))
        if os.path.isfile(patch_path):
            # use patch file to create patched version of cleaned file
            patch_law(law_num, law_year)

        try:
            make_xml(law_num, law_year)

        except BaseException:
            # Git is used to monitor success of parsing new versions of the legal
            # codex. When we fail at parsing a document, the accurate
            # representation is the absence of that document.
            try:
                os.remove(XML_FILENAME % (law_year, law_num))
            except FileNotFoundError:
                # This does not matter.
                pass

            # Make sure that this ends up being someone's problem.
            raise

        # Report back that law was processed without error.
        return [law_id, None]

    except Exception:
        # Something went wrong in processing the law, so we'll record the
        # traceback to the errormap.
        import traceback

        lines = "".join(traceback.format_exception(*sys.exc_info()))
        print("Error processing law %s: %s" % (law_id, lines))
        # Report back which law was attempted to process, and error.
        return [law_id, lines]


def get_available_law_ids():
    law_ids = []
    for filename in os.listdir(os.path.dirname(LAW_FILENAME)):
        if re.match(r"^\d{7}\.html$", filename):
            law_year = int(filename[0:4])
            law_num = int(filename[4:7])
            law_id = "%d/%d" % (law_num, law_year)
            law_ids.append(law_id)
    return law_ids


def get_broken_law_ids():
    law_ids = []

    if os.path.isfile(ERRORMAP_FILENAME):
        with open(ERRORMAP_FILENAME, "r") as f:
            errormap = json.load(f, object_pairs_hook=OrderedDict)
            for law_id in errormap.keys():
                if errormap[law_id] is not None:
                    law_ids.append(law_id)

    return law_ids


# Displays what's going on to the terminal.
def report(law_id, i, law_count, msg):
    nr = str(i + 1)
    while len(nr) < 4:
        nr = " %s" % nr

    law_id_str = str(law_id)
    while len(law_id_str) < 8:
        law_id_str = " %s" % law_id_str

    if msg == "done":
        color = Fore.GREEN
    elif msg == "failed":
        color = Fore.RED
    else:
        raise Exception('Unknown message "%s"' % msg)

    print(
        "[%s/%d] %s %s%s%s" % (nr, law_count, law_id_str, color, msg, Style.RESET_ALL)
    )


# Displays errors known according to errormap.
def display_errors(law_ids):
    with open(ERRORMAP_FILENAME, "r") as f:
        errormap = json.load(f, object_pairs_hook=OrderedDict)

        # Counters.
        failures = 0
        successes = 0

        # The way we want to display this is basically the opposite of
        # how we want to store it. We shall invert the errormap.
        inverted_errormap = OrderedDict()
        for law_id in errormap.keys():
            # Ignore errors that are not a part of the laws currently
            # being requested for processing.
            if law_id not in law_ids:
                continue

            error_msg = errormap[law_id]

            if error_msg:
                failures += 1
            else:
                # If there is no error, it's success.
                successes += 1
                continue

            if error_msg not in inverted_errormap:
                inverted_errormap[error_msg] = []

            inverted_errormap[error_msg].append(law_id)

        # Sort errors so that the most common ones are shown last.
        inverted_errormap = OrderedDict(
            sorted(inverted_errormap.items(), key=lambda x: len(x[1]))
        )

        width, height = terminal_width_and_height()
        for error_msg in inverted_errormap.keys():
            print()
            print("%s" % ("-" * width))
            print(error_msg)
            print("Errors: %d" % len(inverted_errormap[error_msg]))
            print()
            for law_id in inverted_errormap[error_msg]:
                print(" - %s" % law_id, end="")

        print()
        print("%s" % ("-" * width))
        print(
            "Total: %d. Successes: %d. Failures: %d. Success ratio: %.2f%%."
            % (
                successes + failures,
                successes,
                failures,
                100 * (successes / (successes + failures)),
            )
        )


def update_index():
    print("Updating index...", end="", flush=True)

    # Index root element.
    index = E("index")

    # A place for statistics and such. These will be turned into XML nodes
    # later on but are consolidated here for clarity.
    stats = {
        "total-count": 0,
        "empty-count": 0,
        "non-empty-count": 0,
    }

    # Add a place to place the law entries.
    law_entries = E("law-entries")
    index.append(law_entries)

    # Iterate through all the XML files we have.
    for law_xml_filename in os.listdir(os.path.dirname(XML_FILENAME)):
        # Only interested in XML files.
        if law_xml_filename[-4:] != ".xml":
            continue

        # Determine the law's year and number from the XML file's name.
        try:
            law_year = int(law_xml_filename[0:4])
            law_nr = law_xml_filename[5 : law_xml_filename.find(".", 5)]
        except ValueError:
            # This is not a file in the correct name format. Probably the
            # index itself. We'll ignore it.
            continue

        # Read the XML file's content, from which we'll get a bunch of
        # information for the index.
        law = etree.parse(XML_FILENAME % (law_year, law_nr)).getroot()

        # Law-specific info for the index.
        identifier = f"{law_nr}/{law_year}"
        name = law.find("name").text
        date = law.find("num-and-date/date").text
        is_empty = law.getchildren()[-1].tag == "minister-clause"
        art_count = len(law.findall("art"))

        # Determine some structural statistics.
        chapter_count = len(law.xpath("/law/chapter"))
        subchapter_count = len(law.xpath("/law/chapter/subchapter"))
        art_count = len(law.xpath("/law/art"))
        if chapter_count > 0:
            art_count += len(law.xpath("/law/chapter/art"))
        if subchapter_count > 0:
            art_count += len(law.xpath("/law/chapter/subchapter/art"))

        # Determine the document's structure from the stats we've found.
        # NOTE: This method of consecutively appending path elements to the
        # `chapter_structure` may very well need to be revised when we begin
        # to support sub-chapters, super-chapters, different kinds of
        # chapters, since we will almost certainly run into much more
        # complicated scenarios than those that can be described in such a
        # hierarchical fashion.
        #
        # For example, lög nr. 42/1983 (152c) has a chapter for temporary
        # clauses, but is otherwise without chapters.
        structure = ""  # Undetermined
        if chapter_count > 0:
            structure += "/chapter"
        if subchapter_count > 0:
            structure += "/subchapter"
        if art_count > 0:
            structure += "/art"

        # Generate conjugated names of law.
        #
        # Most laws start with the string "Lög um" with everything following
        # already being in the accusative, so we really only need to conjugate
        # the "Lög um", which we can hard-code here since it's so predictable
        # within our linguistic context. External libraries are actually more
        # likely to make mistakes because they are unaware of the context, so
        # they may not even recognize the gender of "lög", for example, and may
        # conjugate parts of the law that we already know from context should
        # be in the accusative. Also, this is **way** faster than invoking all
        # the complications and corner-cases of Icelandic grammar.
        #
        # In other cases, the name ends with "lög" which we will conjugate
        # using an external library because those conjugations may include
        # complicated grammatical rules that we don't want to write ourselves.
        conjugation_success = False
        if name.find("Lög um") == 0:
            # The most common and predictable form.

            rest_of_name = name[7:]
            name_accusative = "Lög um %s" % rest_of_name
            name_dative = "Lögum um %s" % rest_of_name
            name_genitive = "Laga um %s" % rest_of_name

            conjugation_success = True
        else:
            # Things are a bit complicated now. Invoking external library.
            name_phrase = NounPhrase(name)
            name_accusative = name_phrase.accusative
            name_dative = name_phrase.dative
            name_genitive = name_phrase.genitive

            # At the time of this writing (2024-02-29, version 153c) only 3
            # examples of failed conjugations exist:
            #
            # - Tilskipun um fardaga presta á Íslandi og um réttindi þau, er
            #   prestur sá, sem frá brauði fer, eður erfingjar hans og einkum
            #   ekkjan eiga heimting á
            #
            # - Lög viðvíkjandi nafnbreyting Vinnuveitendafélags Íslands
            #
            # - Konungsbréf (til stiftamtm. og amtm.) um fiskiútveg á Íslandi
            #
            # All of them are empty or irrelevant so we won't deal with them
            # further. If necessary, they can be hard-coded since new laws with
            # this problem are extremely unlikely to ever come into effect.
            conjugation_success = name_phrase.parsed

        # We also make the first letter of the conjugated form lowercase to
        # make comparison easier, since conjugated forms are never in the
        # beginning of a sentence.
        if conjugation_success:
            name_accusative = name_accusative[0].lower() + name_accusative[1:]
            name_dative = name_dative[0].lower() + name_dative[1:]
            name_genitive = name_genitive[0].lower() + name_genitive[1:]

        # Append the gathered data to the index.
        law_entry = E(
            "law-entry",
            {
                "identifier": identifier,
                "nr": law_nr,
                "year": str(law_year),
                "date": date,
            },
            E("name", name),
            E(
                "name-conjugated",
                {
                    "success": "true" if conjugation_success else "false",
                },
                E("nomenative", name),
                E("accusative", name_accusative or ""),
                E("dative", name_dative or ""),
                E("genitive", name_genitive or ""),
            ),
            E(
                "meta",
                E("is-empty", "true" if is_empty else "false"),
                E("structure", structure),
                E("chapter-count", str(chapter_count)),
                E("art-count", str(art_count)),
            ),
        )

        law_entries.append(law_entry)
        print(".", end="", flush=True)

        # Add to stats. We are not sure what the rendering client is
        # interested in, so we'll leave a few variables for convenience, even
        # if they are easily calculable. The client should not need to
        # calculate basic things.
        stats["total-count"] += 1
        if is_empty:
            stats["empty-count"] += 1
        else:
            stats["non-empty-count"] += 1

    def number_sorter(number):
        """
        Used for properly sorting numbers in a string, so that "7" comes before
        "70" by prepending zeroes.
        """
        result = str(number)
        while len(result) < 3:
            result = "0%s" % result
        return result

    # Sort the law entries by year and number.
    law_entries[:] = reversed(
        sorted(
            law_entries,
            key=lambda law: (
                law.attrib["year"],
                number_sorter(law.attrib["nr"]),
            ),
        )
    )

    # Insert statistics into XML.
    node_stats = E("stats")
    index.insert(0, node_stats)
    for stat_key in stats.keys():
        node_stats.append(E(stat_key, str(stats[stat_key])))

    write_xml(index, XML_INDEX_FILENAME, skip_prettyprint_hack=True)

    print(" done")


def usage(exec_name, message=None):
    print(
        "Usage: %s [law_number>/<year>] [law_number>/<year>]..." % exec_name,
        file=stderr,
    )
    print(file=stderr)
    print(
        "Running without options or specific laws will process all available laws.",
        file=stderr,
    )
    print(file=stderr)
    print("Options:", file=stderr)
    print("    --help                    Display this help message.", file=stderr)
    print(file=stderr)
    print(
        "    --try-broken              Try processing laws known to have failed.",
        file=stderr,
    )
    print(
        "    --single-thread           Skip multi-processing (mostly for debugging).",
        file=stderr,
    )
    print(
        "    --rebuild-straytextmap    Force the asking of user about stray text following numarts",
        file=stderr,
    )
    print(file=stderr)
    print("Index options:", file=stderr)
    print(
        "    --index-only              Don't process laws or parse references, only update the index.",
        file=stderr,
    )
    print(
        "    --skip-index              Skip updating index.",
        file=stderr,
    )
    print(file=stderr)
    print("Reference parsing options:", file=stderr)
    print(
        "    --references-only         Don't process laws or update index, only parse references.",
        file=stderr,
    )
    print("    --skip-references         Skip the parsing of references.", file=stderr),
    print(file=stderr)
    if message:
        print("Error: %s" % message, file=stderr)
    quit(1)


def main(argv):
    # A list of valid options so that we'll know when something is thrown in
    # that we don't know what to do with.
    valid_options = [
        "--help",
        "--try-broken",
        "--single-thread",
        "--rebuild-straytextmap",
        "--index-only",
        "--skip-index",
        "--references-only",
        "--skip-references",
        "--verbose"
    ]

    if "--verbose" in argv:
        global VERBOSE_MODE
        VERBOSE_MODE = True

    if "--help" in argv:
        usage(argv[0])

    # Only update the index and immediately quit if so requested.
    if "--index-only" in argv:
        update_index()
        quit()
    elif "--references-only" in argv:
        parse_references()
        quit()

    # Container for laws that are about to be processed.
    law_ids = []

    if "--try-broken" in argv:
        law_ids = get_broken_law_ids()

    # Add things in command line that match the pattern for a law_id.
    for arg in argv[1:]:
        if re.match(r"^\d{1,4}\/\d{4}$", arg):
            law_ids.append(arg)
        elif arg in valid_options:
            settings.options[arg] = True
        elif arg not in valid_options:
            usage(argv[0], 'Unknown option "%s"' % arg)

    # If nothing is selected, we'll process everything.
    if len(law_ids) == 0:
        law_ids = get_available_law_ids()

    # Sort the law_ids in their own special way (year first, num second).
    law_ids = sorted_law(law_ids)

    # This is apparently safer than `multiprocessing.cpu_count()`,
    # according to:
    # https://stackoverflow.com/questions/1006289/how-to-find-out-the-number-of-cpus-using-python
    cpu_count = len(os.sched_getaffinity(0))

    def init_pool():
        # Start ignoring the KeyboardInterrupt signal in the main thread. The
        # result is that it gets caught by the sub-processes, which **don't**
        # inherit this setting. The exception is then thrown when waiting for
        # the process pool to finish, and caught by the code running the
        # `main` function.
        signal.signal(signal.SIGINT, signal.SIG_IGN)

    with Pool(cpu_count, init_pool) as pool:
        if "--single-thread" in argv:
            # Multiprocessing has some wide-reaching implications for various
            # under-the-hood mechanics like debugging. We need the ability to
            # have those work, so we offer this option to sidestep threading.
            #
            # This is only "yield-ified" so that the results stay compatible
            # with the code that handles those same results when using
            # multiprocessing.
            def yieldify_processing(law_ids):
                for law_id in law_ids:
                    yield process_law(law_id)

            results = yieldify_processing(law_ids)
        else:
            # Start an asynchronous pool of processes, as many as there are
            # CPUS, giving them a list of the laws that need processing.
            results = pool.imap_unordered(process_law, law_ids)

        # Open the errormap for recording successes and errors.
        with open(ERRORMAP_FILENAME, "r") as f:
            errormap = json.load(f, object_pairs_hook=OrderedDict)

        # Monitor and record return successes and errors. We'll want to record
        # the errormap, no matter what.
        try:
            # Initial state of iterator.
            i = 0

            # Let's only do this once.
            law_count = len(law_ids)

            # Keep doing this until we hit a StopIteration exception.
            while True:
                try:
                    # Catch next result.
                    law_id, error_trace = next(results)

                    # Remember in errormap (gets written later).
                    errormap[law_id] = error_trace

                    # Tell the user about it.
                    msg = "done" if error_trace is None else "failed"
                    report(law_id, i, law_count, msg)

                    # Increase iterator.
                    i += 1

                except StopIteration:
                    break
        finally:
            # Write the errormap.
            with open(ERRORMAP_FILENAME, "w") as f:
                json.dump(errormap, f)

    # Update the index (unless skipped).
    if "--skip-index" not in argv:
        update_index()

    # Parse references (unless skipped).
    if "--skip-references" not in argv:
        parse_references()

    # List the errors and identify laws in which they occurred.
    display_errors(law_ids)


try:
    main(sys.argv)
except KeyboardInterrupt:
    quit()
except Exception as e:
    if settings.DEBUG:
        raise
    else:
        print("Error: %s" % e, file=stderr)
