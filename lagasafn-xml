#!/usr/bin/env python3
import codecs
import cloudscraper
import json
import re
import os
import sys
from bs4 import BeautifulSoup
from collections import OrderedDict
from colorama import Fore
from colorama import Style
from datetime import datetime
from datetime import timedelta
from lagasafn import settings
from lagasafn.constants import JSON_MAP_BASE_DIR
from lagasafn.constants import LAW_FILENAME
from lagasafn.constants import LAW_FILENAME_DIR
from lagasafn.constants import ERRORMAP_FILENAME
from lagasafn.constants import CLEAN_FILENAME
from lagasafn.constants import PATCHED_FILENAME
from lagasafn.constants import PATCHES_BASE_DIR
from lagasafn.constants import SPLITMAP_FILENAME
from lagasafn.constants import STRAYTEXTMAP_FILENAME
from lagasafn.constants import XML_BASE_DIR
from lagasafn.constants import XML_INDEX_FILENAME
from lagasafn.constants import XML_FILENAME
from lagasafn.constants import XML_FILENAME_DIR
from lagasafn.contenthandlers import generate_conjugations
from lagasafn.contenthandlers import generate_synonyms
from lagasafn.contenthandlers import remove_ignorables
from lagasafn.exceptions import LawException
from lagasafn.multiprocessing import CustomPool
from lagasafn.patching import attempt_json_map_transfer
from lagasafn.patching import patch_law
from lagasafn.population import population_game
from lagasafn.references import parse_references
from lagasafn.settings import CURRENT_PARLIAMENT_VERSION
from lagasafn.utils import determine_month
from lagasafn.utils import number_sorter
from lagasafn.utils import sorted_law
from lagasafn.utils import terminal_width_and_height
from lagasafn.utils import write_xml
from lxml import etree
from lxml.builder import E
from lxml.html import soupparser
from multiprocessing import Pool
from sys import stderr
from lagasafn.parser import LawParser, parse_law
from os.path import dirname
from os.path import isdir
from os.path import isfile

VERBOSE_MODE = False


def prepare_environment():
    """
    Prepares the environment for a new codex version.
    """

    def ensure_dir(dirname):
        if not isdir(dirname):
            os.makedirs(dirname)

    json_files = [
        ERRORMAP_FILENAME % CURRENT_PARLIAMENT_VERSION,
        STRAYTEXTMAP_FILENAME % CURRENT_PARLIAMENT_VERSION,
        SPLITMAP_FILENAME % CURRENT_PARLIAMENT_VERSION,
    ]

    dirs = [
        os.path.join(XML_BASE_DIR, CURRENT_PARLIAMENT_VERSION),
        os.path.join(PATCHES_BASE_DIR, CURRENT_PARLIAMENT_VERSION),
        os.path.join(JSON_MAP_BASE_DIR, CURRENT_PARLIAMENT_VERSION),
    ]
    for dir in dirs:
        ensure_dir(dir)

    if "--auto-patch" in settings.options:
        attempt_json_map_transfer()

    for json_file in json_files:
        ensure_dir(dirname(json_file))
        if not isfile(json_file):
            with open(json_file, "w") as f:
                f.write("{}")


def clean_content(content):

    if not settings.FEATURES["PARSE_MARKERS"]:
        # Remove markers for previous changes and removed content.
        content = content.replace("[", "").replace("]", "")
        content = content.replace("…", "").replace("&hellip;", "")

    # Remove superscript/subscript when ratios are presented in the form of
    # divisions. For example, "3/4" tends to be stylized with a superscripted
    # "3" and a subscripted "4". We'll want to remove such styling because
    # we're only interested in content. Layouting mechanisms will have to
    # stylize them again if needed.
    e = re.compile(
        r'<sup style="font-size:60%">(\d+)</sup>/<span style="font-size:60%">(\d+)</span>'
    )
    content = e.sub(r"\1/\2", content)

    # Chemicals, distances and such are styled in the HTML itself. We want to
    # remove styling from them and instead have the rendering mechanism style
    # them properly.
    styled_abbreviations = [
        ["CO<small><sub>2</sub></small>", "CO2"],
        ["CO<small><sub>3</sub></small>", "CO3"],
        ["SO<small><sub>2</sub></small>", "SO2"],
        ["SF<small><sub>6</sub></small>", "SF6"],
        ["Na<small><sub>2</sub></small>", "Na2"],
        ["NaHCO<small><sub>3</sub></small>", "NaHCO3"],
        ["NF<small><sub>3</sub></small>", "NF3"],
        ["CH<small><sub>4</sub></small>", "CH4"],
        ["N<small><sub>2</sub></small>", "N2"],
        ["H<small><sub>2</sub></small>", "H2"],
        ["m<small><sup>2</sup></small>", "m2"],
        ["m<small><sup>3</sup></small>", "m3"],
    ]
    for styled, unstyled in styled_abbreviations:
        content = content.replace(styled, unstyled)

    # Remove links to other laws. These are not useful in their current state.
    # Rather, references to other laws, such as "laga nr. XX/XXXX" or "lög nr.
    # XX/XXXX" should be automatically turned into precise references. (This
    # has not been implemented at the time of this writing. Remove this
    # comment when it has been. 2021-06-25)
    e = re.compile(r'<a href="\d{7}.html">(.*?)</a>')
    content = e.sub(r"\1", content)

    # In 3. mgr. 17. gr. laga nr. 6/2001, we have a unique marking with a star
    # ("*") that seems to indicate the legal context of the change. It is
    # entirely unclear (as of 2024-12-04) how this is supposed to work. We will
    # clean it for now, in a rather brutish way, until it is clearer if, and
    # how, these things will be handled in the future.
    #
    # This takes care both of the superscripted star in the text as well as we
    # the corresponding bit in the footer, which otherwise gets mixed with
    # another footnote.
    content = re.sub(
        r'<sup style="font-size:60%">\*<\/sup>(sbr\. <a href="\/altext\/stjt\/\d{4}.\d{3}.html">\d+\. gr\.<\/a>)?',
        '',
        content
    )

    # Find the law content and exit if it does not exist
    soup_law = BeautifulSoup(content, "html5lib").find("html")
    if soup_law is None:
        return None

    soup = BeautifulSoup(soup_law.__str__(), "html5lib")  # Parse the law content

    if not settings.FEATURES["PARSE_MARKERS"]:
        # Remove superscripts indicating previous change. Only removes them when
        # they are found outside of footnotes.
        superscripts = soup.find_all("sup")
        for s in superscripts:
            if s.parent.name != "small" and re.match(r"^\d{1,3}\)$", s.text):
                s.extract()

    # Remove empty tags, but only if they're empty
    empty_tags_to_remove = ["p", "h2", "i"]
    for target_tag in empty_tags_to_remove:
        empty_tags = soup.find_all(
            lambda tag: tag.name == target_tag
            and not tag.contents
            and (tag.string is None or not tag.string.strip())
        )
        [empty_tag.extract() for empty_tag in empty_tags]

    # Add charset tag
    charset_tag = soup.new_tag("meta", charset="utf-8")
    soup.insert(0, charset_tag)

    # Remove ignorables such as appendices and external documents that don't
    # belong to the law itself. The nature and format of these things are not
    # feasibly parsable.
    soup = remove_ignorables(soup)

    xhtml = soup.prettify()

    # Final cleanup at text-stage.
    xhtml = xhtml.replace(" <!-- Tab -->\n  ", "&nbsp;&nbsp;&nbsp;&nbsp;")

    return xhtml


def clean_law(law_num, law_year):
    with codecs.open(
    LAW_FILENAME % (CURRENT_PARLIAMENT_VERSION, law_year, str(law_num).zfill(3)),
        "r",
        "ISO-8859-1"
    ) as infile:
        raw_content = infile.read()
        infile.close()

    content = clean_content(raw_content)

    if content is None:
        print(" failed.")
        print("Error: Law %d/%d does not seem to exist" % (law_year, law_num))
        quit(1)

    if not os.path.isdir(dirname(CLEAN_FILENAME)):
        os.mkdir(dirname(CLEAN_FILENAME))

    with open(CLEAN_FILENAME % (law_year, law_num), "w") as clean_file:
        clean_file.write(content)
        clean_file.close()


def make_xml(law_num, law_year):
    parser = LawParser(law_num, law_year)
    global VERBOSE_MODE
    if VERBOSE_MODE:
        print("Parsing law %d/%d" % (law_num, law_year))
        parser.verbosity = 1

    parse_law(parser)

    # Write the XML object to output file.
    write_xml(parser.law, XML_FILENAME % (CURRENT_PARLIAMENT_VERSION, parser.law_year, parser.law_num))


def process_law(law_id):
    try:
        (law_num, law_year) = law_id.split("/")
        law_num = int(law_num)
        law_year = int(law_year)

        clean_law(law_num, law_year)

        # Delete existing patch file so we don't accidentally have outdated
        # patched files when for example changing to another parliament version.
        patched_path = PATCHED_FILENAME % (law_year, law_num)
        if os.path.isfile(patched_path):
            os.unlink(patched_path)

        # Apply patch to law if available.
        patch_law(law_num, law_year)

        try:
            make_xml(law_num, law_year)

        except BaseException:
            # Git is used to monitor success of parsing new versions of the legal
            # codex. When we fail at parsing a document, the accurate
            # representation is the absence of that document.
            try:
                os.remove(XML_FILENAME % (CURRENT_PARLIAMENT_VERSION, law_year, law_num))
            except FileNotFoundError:
                # This does not matter.
                pass

            # Make sure that this ends up being someone's problem.
            raise

        # Report back that law was processed without error.
        return [law_id, None]

    except Exception:
        # Something went wrong in processing the law, so we'll record the
        # traceback to the errormap.
        import traceback

        lines = "".join(traceback.format_exception(*sys.exc_info()))
        print("Error processing law %s: %s" % (law_id, lines))
        # Report back which law was attempted to process, and error.
        return [law_id, lines]


def get_available_law_ids():
    law_ids = []
    for filename in os.listdir(LAW_FILENAME_DIR % CURRENT_PARLIAMENT_VERSION):
        if re.match(r"^\d{7}\.html$", filename):
            law_year = int(filename[0:4])
            law_num = int(filename[4:7])
            law_id = "%d/%d" % (law_num, law_year)
            law_ids.append(law_id)
    return law_ids


def get_broken_law_ids():
    law_ids = []

    if os.path.isfile(ERRORMAP_FILENAME % CURRENT_PARLIAMENT_VERSION):
        with open(ERRORMAP_FILENAME % CURRENT_PARLIAMENT_VERSION, "r") as f:
            errormap = json.load(f, object_pairs_hook=OrderedDict)
            for law_id in errormap.keys():
                if errormap[law_id] is not None:
                    law_ids.append(law_id)

    return law_ids


# Displays what's going on to the terminal.
def report(law_id, i, law_count, msg):
    nr = str(i + 1)
    while len(nr) < 4:
        nr = " %s" % nr

    law_id_str = str(law_id)
    while len(law_id_str) < 8:
        law_id_str = " %s" % law_id_str

    if msg == "done":
        color = Fore.GREEN
    elif msg == "failed":
        color = Fore.RED
    else:
        raise Exception('Unknown message "%s"' % msg)

    print(
        "[%s/%d] %s %s%s%s" % (nr, law_count, law_id_str, color, msg, Style.RESET_ALL)
    )


# Displays errors known according to errormap.
def display_errors(law_ids):
    with open(ERRORMAP_FILENAME % CURRENT_PARLIAMENT_VERSION, "r") as f:
        errormap = json.load(f, object_pairs_hook=OrderedDict)

        # Counters.
        failures = 0
        successes = 0

        # The way we want to display this is basically the opposite of
        # how we want to store it. We shall invert the errormap.
        inverted_errormap = OrderedDict()
        for law_id in errormap.keys():
            # Ignore errors that are not a part of the laws currently
            # being requested for processing.
            if law_id not in law_ids:
                continue

            error_msg = errormap[law_id]

            if error_msg:
                failures += 1
            else:
                # If there is no error, it's success.
                successes += 1
                continue

            if error_msg not in inverted_errormap:
                inverted_errormap[error_msg] = []

            inverted_errormap[error_msg].append(law_id)

        # Sort errors so that the most common ones are shown last.
        inverted_errormap = OrderedDict(
            sorted(inverted_errormap.items(), key=lambda x: len(x[1]))
        )

        width, height = terminal_width_and_height()
        for error_msg in inverted_errormap.keys():
            print()
            print("%s" % ("-" * width))
            print(error_msg)
            print("Errors: %d" % len(inverted_errormap[error_msg]))
            print()
            for law_id in inverted_errormap[error_msg]:
                print(" - %s" % law_id, end="")

        print()
        print("%s" % ("-" * width))
        print(
            "Total: %d. Successes: %d. Failures: %d. Success ratio: %.2f%%."
            % (
                successes + failures,
                successes,
                failures,
                100 * (successes / (successes + failures)),
            )
        )


def get_parliament_version_dates(codex_version):
    """
    Each version of a codex (a.k.a. parliament version) has a date associated
    with it, which is up to what date it incorporates changes passed by
    Parliament. For example, codex version 154c contains changes up to
    September 1st, 2024. Version 154b contained changes up to April 12th, 2024,
    and so version 154c should contain changes advertised by the gazette (i.
    stjórnartíðindi) from August 13th to September 1st of 2024.

    We'll use these dates to match adverts with a codex version.

    At the time of this writing (2025-01-25), this information is not known to
    be available anywhere except in the download section of Althingi's website,
    so we'll just scrape that and place it in the index.
    """

    # Scraping Cloudflare is error-prone to say the least, and is unlikely to
    # get easier since their feature is to prevent programs from downloading
    # content from the web page. However, these dates never change, so when
    # we're re-creating the index here, we'll allow ourselves to check if we
    # already have this data, and re-use it if it's available.
    try:
        xml_index = etree.parse(XML_INDEX_FILENAME % codex_version).getroot()
        date_from = datetime.fromisoformat(xml_index.attrib["date-from"])
        date_to = datetime.fromisoformat(xml_index.attrib["date-to"])
        return date_from, date_to
    except Exception:
        # If this fails for any reason, we'll want to move on.
        pass

    url = "https://www.althingi.is/lagasafn/zip-skra-af-lagasafni/"
    scraper = cloudscraper.create_scraper()
    response = scraper.get(url)
    xml = soupparser.fromstring(response.content)

    if response.status_code != 200:
        raise LawException("Failed finding date range of legal codex. This usually happens when Althingi's website denies the necessary HTTP request. Try waiting and trying with `--index-only` again.")

    # Find the links to the current codex version and the one preceding it.
    link = xml.xpath(
        "//a[@href='/lagasafn/zip/%s/allt.zip']" % CURRENT_PARLIAMENT_VERSION
    )[0]
    prev_link = link.getparent().getnext().find("a")

    # Read dates from "to" date.
    day, month, year = re.match(r"\d{3}[a-z]?\. Íslensk lög (\d+)\. (.+) (\d{4})", link.text).groups()
    date_to = datetime(int(year), determine_month(month), int(day))

    # Read dates from "from" date.
    day, month, year = re.match(r"\d{3}[a-z]?\. Íslensk lög (\d+)\. (.+) (\d{4})", prev_link.text).groups()
    date_from = datetime(int(year), determine_month(month), int(day))

    # The first day of this version is the one following the last day of the
    # last version.
    date_from += timedelta(days=1)

    return date_from, date_to


def update_index():
    print("Updating index...", end="", flush=True)

    date_from, date_to = get_parliament_version_dates(CURRENT_PARLIAMENT_VERSION)

    # Index root element.
    index = E("index", {
        "date-from": date_from.strftime("%Y-%m-%d"),
        "date-to": date_to.strftime("%Y-%m-%d"),
    })

    # A place for statistics and such. These will be turned into XML nodes
    # later on but are consolidated here for clarity.
    stats = {
        "total-count": 0,
        "empty-count": 0,
        "non-empty-count": 0,
    }

    # Add a place to place the law entries.
    law_entries = E("law-entries")
    index.append(law_entries)

    # Iterate through all the XML files we have.
    for law_xml_filename in os.listdir(XML_FILENAME_DIR % CURRENT_PARLIAMENT_VERSION):
        # Only interested in XML files.
        if law_xml_filename[-4:] != ".xml":
            continue

        # Determine the law's year and number from the XML file's name.
        try:
            law_year = int(law_xml_filename[0:4])
            law_nr = law_xml_filename[5 : law_xml_filename.find(".", 5)]
        except ValueError:
            # This is not a file in the correct name format. Probably the
            # index itself. We'll ignore it.
            continue

        # Read the XML file's content, from which we'll get a bunch of
        # information for the index.
        law = etree.parse(XML_FILENAME % (CURRENT_PARLIAMENT_VERSION, law_year, law_nr)).getroot()

        # Law-specific info for the index.
        identifier = f"{law_nr}/{law_year}"
        name = law.find("name").text
        date = law.find("num-and-date/date").text
        is_empty = law.getchildren()[-1].tag == "minister-clause"
        art_count = len(law.findall("art"))

        # Determine some structural statistics.
        chapter_count = len(law.xpath("/law/chapter"))
        subchapter_count = len(law.xpath("/law/chapter/subchapter"))
        art_count = len(law.xpath("/law/art"))
        if chapter_count > 0:
            art_count += len(law.xpath("/law/chapter/art"))
        if subchapter_count > 0:
            art_count += len(law.xpath("/law/chapter/subchapter/art"))

        # Determine the document's structure from the stats we've found.
        # NOTE: This method of consecutively appending path elements to the
        # `chapter_structure` may very well need to be revised when we begin
        # to support sub-chapters, super-chapters, different kinds of
        # chapters, since we will almost certainly run into much more
        # complicated scenarios than those that can be described in such a
        # hierarchical fashion.
        #
        # For example, lög nr. 42/1983 (152c) has a chapter for temporary
        # clauses, but is otherwise without chapters.
        structure = ""  # Undetermined
        if chapter_count > 0:
            structure += "/chapter"
        if subchapter_count > 0:
            structure += "/subchapter"
        if art_count > 0:
            structure += "/art"

        # Append the gathered data to the index.
        conjugations = generate_conjugations(name)
        law_entry = E(
            "law-entry",
            {
                "identifier": identifier,
                "nr": law_nr,
                "year": str(law_year),
                "date": date,
            },
            E("name", name),
            E(
                "name-conjugated",
                E("nomenative", name),
                E("accusative", conjugations["accusative"]),
                E("dative", conjugations["dative"]),
                E("genitive", conjugations["genitive"]),
            ),
            E(
                "meta",
                E("is-empty", "true" if is_empty else "false"),
                E("structure", structure),
                E("chapter-count", str(chapter_count)),
                E("art-count", str(art_count)),
            ),
        )

        # Generate synonym elements, if appropriate.
        elem_synonyms = None
        synonyms = generate_synonyms(name)
        if len(synonyms) > 0:

            if elem_synonyms is None:
                elem_synonyms = E("synonyms")

            for synonym in synonyms:
                elem_synonyms.append(
                    E(
                        "synonym",
                        E("nomenative", synonym["nomenative"]),
                        E("accusative", synonym["accusative"]),
                        E("dative", synonym["dative"]),
                        E("genitive", synonym["genitive"]),
                    )
                )
        if elem_synonyms is not None:
            # Insert it after the name.
            law_entry.insert(2, elem_synonyms)

        law_entries.append(law_entry)
        print(".", end="", flush=True)

        # Add to stats. We are not sure what the rendering client is
        # interested in, so we'll leave a few variables for convenience, even
        # if they are easily calculable. The client should not need to
        # calculate basic things.
        stats["total-count"] += 1
        if is_empty:
            stats["empty-count"] += 1
        else:
            stats["non-empty-count"] += 1

    # Sort the law entries by year and number.
    law_entries[:] = reversed(
        sorted(
            law_entries,
            key=lambda law: (
                law.attrib["year"],
                number_sorter(law.attrib["nr"]),
            ),
        )
    )

    # Insert statistics into XML.
    node_stats = E("stats")
    index.insert(0, node_stats)
    for stat_key in stats.keys():
        node_stats.append(E(stat_key, str(stats[stat_key])))

    write_xml(index, XML_INDEX_FILENAME % CURRENT_PARLIAMENT_VERSION)

    print(" done")


def usage(exec_name, message=None):
    print(
        "Usage: %s [law_number>/<year>] [law_number>/<year>]..." % exec_name,
        file=stderr,
    )
    print(file=stderr)
    print(
        "Running without options or specific laws will process all available laws.",
        file=stderr,
    )
    print(file=stderr)
    print("Options:", file=stderr)
    print("    --help                    Display this help message.", file=stderr)
    print(
        "    --verbose                 Show a lot more information about what's going on.",
        file=stderr
    )
    print(file=stderr)
    print(
        "    --population-game         Run the population game, a fun developer tool.",
        file=stderr
    )
    print(
        "    --try-broken              Try processing laws known to have failed.",
        file=stderr,
    )
    print(
        "    --single-thread           Skip multi-processing (mostly for debugging).",
        file=stderr,
    )
    print(
        "    --rebuild-straytextmap    Force the asking of user about stray text following numarts",
        file=stderr,
    )
    print(
        "    --auto-patch              Attempt to auto-patch by patches in other versions of codex.",
        file=stderr,
    )
    print(file=stderr)
    print("Index options:", file=stderr)
    print(
        "    --index-only              Don't process laws or parse references, only update the index.",
        file=stderr,
    )
    print(
        "    --skip-index              Skip updating index.",
        file=stderr,
    )
    print(file=stderr)
    print("Reference parsing options:", file=stderr)
    print(
        "    --references-only         Don't process laws or update index, only parse references.",
        file=stderr,
    )
    print("    --skip-references         Skip the parsing of references.", file=stderr)

    print(file=stderr)
    if message:
        print("Error: %s" % message, file=stderr)
    quit(1)


def main(argv):
    # A list of valid options so that we'll know when something is thrown in
    # that we don't know what to do with.
    valid_options = [
        "--help",
        "--try-broken",
        "--single-thread",
        "--rebuild-straytextmap",
        "--index-only",
        "--skip-index",
        "--references-only",
        "--skip-references",
        "--verbose",
        "--population-game",
        "--auto-patch",
    ]

    if "--verbose" in argv:
        global VERBOSE_MODE
        VERBOSE_MODE = True

    if "--help" in argv:
        usage(argv[0])

    # Container for laws that are about to be processed.
    law_ids = []

    if "--try-broken" in argv:
        law_ids = get_broken_law_ids()

    # Add things in command line that match the pattern for a law_id.
    for arg in argv[1:]:
        if re.match(r"^\d{1,4}\/\d{4}$", arg):
            law_ids.append(arg)
        elif arg in valid_options:
            settings.options[arg] = True
        elif arg not in valid_options:
            usage(argv[0], 'Unknown option "%s"' % arg)

    prepare_environment()

    # If nothing is selected, we'll process everything.
    if len(law_ids) == 0:
        law_ids = get_available_law_ids()

    # Sort the law_ids in their own special way (year first, num second).
    law_ids = sorted_law(law_ids)

    # Only update the index and immediately quit if so requested.
    if "--index-only" in argv:
        update_index()
        quit()
    elif "--references-only" in argv:
        parse_references(law_ids)
        quit()
    elif "--population-game" in argv:
        population_game()
        quit()

    with CustomPool() as pool:
        results = pool.run(process_law, law_ids)

        # Open the errormap for recording successes and errors.
        with open(ERRORMAP_FILENAME % CURRENT_PARLIAMENT_VERSION, "r") as f:
            errormap = json.load(f, object_pairs_hook=OrderedDict)

        # Monitor and record return successes and errors. We'll want to record
        # the errormap, no matter what.
        try:
            # Initial state of iterator.
            i = 0

            # Let's only do this once.
            law_count = len(law_ids)

            # Keep doing this until we hit a StopIteration exception.
            while True:
                try:
                    # Catch next result.
                    law_id, error_trace = next(results)

                    # Remember in errormap (gets written later).
                    errormap[law_id] = error_trace

                    # Tell the user about it.
                    msg = "done" if error_trace is None else "failed"
                    report(law_id, i, law_count, msg)

                    # Increase iterator.
                    i += 1

                except StopIteration:
                    break
        finally:
            # Write the errormap.
            with open(ERRORMAP_FILENAME % CURRENT_PARLIAMENT_VERSION, "w") as f:
                json.dump(errormap, f)

    # Update the index (unless skipped).
    if "--skip-index" not in argv:
        update_index()

    # Parse references (unless skipped).
    if "--skip-references" not in argv:
        parse_references(law_ids)

    # List the errors and identify laws in which they occurred.
    display_errors(law_ids)


if __name__ == "__main__":
    try:
        main(sys.argv)
    except KeyboardInterrupt:
        quit()
    except Exception as e:
        if settings.DEBUG:
            raise
        else:
            print("Error: %s" % e, file=stderr)
